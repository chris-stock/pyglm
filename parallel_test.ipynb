{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from test.parallel_harness import *\n",
      "\n",
      "# Manually load data\n",
      "dataFile = 'data.pkl'\n",
      "with open(dataFile,'r') as f:\n",
      "    data = cPickle.load(f)\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print data stats\n",
      "N = data['N']\n",
      "Ns = np.sum(data['S'])\n",
      "T = data['S'].shape[0]\n",
      "fr = 1.0/data['dt']\n",
      "                \n",
      "print \"Data has %d neurons, %d spikes, \" \\\n",
      "      \"and %d time bins at %.3fHz sample rate\" % \\\n",
      "      (N,Ns,T,fr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Data has 10 neurons, 15371 spikes, and 180000 time bins at 1000.000Hz sample rate\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Creating master population object\"\n",
      "model_type = 'standard_glm'\n",
      "model = make_model(model_type, N=data['N'])\n",
      "popn = Population(model)\n",
      "popn.set_data(data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating master population object\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a client with direct view to all engines\n",
      "client = Client('/home/slinderman/.config/ipython/profile_sge/security/ipcontroller-client.json')\n",
      "dview = client[:]\n",
      "len(dview)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "8"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Initializing imports on each engine\"\n",
      "initialize_imports(dview)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initializing imports on each engine\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Creating population objects on each engine\"\n",
      "create_population_on_engines(dview, data, model_type=model_type)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating population objects on each engine\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure the .theanorc ldflags are being picked up\n",
      "r = dview.execute('import theano; print theano.config', block=True)\n",
      "r.display_outputs()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:0] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x3097578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x3099110>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x30971b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x309b668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x309b7d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x309b938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x309baa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x309bc08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x309bde8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x30a5320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x30a5488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x30a55f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x30a5758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x30a58c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x30a5a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task7\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x3207c90>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task7/compiledir_Linux-2.6.32-279.22.1.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x356c140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x356c398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x3608488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x36085f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x3611e60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x34a4230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x34a4398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x34a4500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x34a4758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x34d1758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x34dd230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x3af8de8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x3896488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x3bae668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:1] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x3475578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x3477110>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x34751b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x3479668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x34797d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x3479938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x3479aa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x3479c08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x3479de8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x3483320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x3483488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x34835f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x3483758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x34838c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x3483a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task3\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x35e5c90>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task3/compiledir_Linux-2.6.32-279.22.1.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x394a140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x394a398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x39e6488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x39e65f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x39efe60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x386c230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x386c398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x386c500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x386c758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x38af758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x38bb230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x3ed8de8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x3c35488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x3f8e668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:2] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x1d3b578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x1d3d150>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x1d3b1b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x1d3f668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x1d3f7d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x1d3f938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x1d3faa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x1d3fc08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x1d3fde8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x1d49320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x1d49488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x1d495f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x1d49758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x1d498c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x1d49a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task2\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x1eabcd0>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task2/compiledir_Linux-2.6.32-358.6.2.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x2213140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x2213398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x22af488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x22af5f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x22b5e60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x1f7a230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x1f7a398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x1f7a500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x1f7a758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x1f84758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x1f90230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x27d8de8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x24e9488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x286b668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:3] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x1f53578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x1f55150>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x1f531b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x1f57668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x1f577d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x1f57938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x1f57aa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x1f57c08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x1f57de8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x1f61320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x1f61488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x1f615f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x1f61758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x1f618c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x1f61a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task6\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x20c3cd0>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task6/compiledir_Linux-2.6.32-358.6.2.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x242b140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x242b398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x24c7488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x24c75f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x24cde60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x218e230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x218e398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x218e500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x218e758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x2198758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x21a4230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x29b6de8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x2713488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x2a78668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:4] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x31f7578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x31f9150>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x31f71b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x31fb668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x31fb7d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x31fb938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x31fbaa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x31fbc08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x31fbde8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x3205320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x3205488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x32055f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x3205758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x32058c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x3205a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task8\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x3368cd0>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task8/compiledir_Linux-2.6.32-358.2.1.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x36cf140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x36cf398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x376b488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x376b5f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x3771e60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x3432230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x3432398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x3432500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x3432758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x343c758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x3448230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x3c5ade8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x39b7488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x3d1b668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:5] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x374a578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x374c110>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x374a1b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x374e668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x374e7d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x374e938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x374eaa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x374ec08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x374ede8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x3758320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x3758488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x37585f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x3758758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x37588c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x3758a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task4\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x38bac90>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task4/compiledir_Linux-2.6.32-358.2.1.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x3c20140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x3c20398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x3cbc488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x3cbc5f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x3cc2e60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x3b80230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x3b80398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x3b80500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x3b80758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x3b87758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x3b91230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x41e8de8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x3ef8488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x427b668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:6] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x28e7578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x28e9110>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x28e71b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x28eb668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x28eb7d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x28eb938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x28ebaa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x28ebc08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x28ebde8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x28f5320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x28f5488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x28f55f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x28f5758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x28f58c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x28f5a28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task1\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x2a57c90>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task1/compiledir_Linux-2.6.32-279.14.1.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x2dbd140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x2dbd398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x2e5b488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x2e5b5f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x2e62e60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x2cd0230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x2cd0398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x2cd0500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x2cd0758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x2cda758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x2ce5230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x334cde8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x30d4488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x3405668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n",
        "[stdout:7] \n",
        "floatX (('float64', 'float32')) \n",
        "    Doc:  Default floating-point precision for python casts\n",
        "    Value:  float64\n",
        "\n",
        "cast_policy (('custom', 'numpy+floatX')) \n",
        "    Doc:  Rules for implicit type casting\n",
        "    Value:  custom\n",
        "\n",
        "int_division (('int', 'raise', 'floatX')) \n",
        "    Doc:  What to do when one computes x / y, where both x and y are of integer types\n",
        "    Value:  int\n",
        "\n",
        "device (('cpu', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Default device for computations. If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.\n",
        "    Value:  cpu\n",
        "\n",
        "init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', 'gpu3', 'gpu4', 'gpu5', 'gpu6', 'gpu7', 'gpu8', 'gpu9', 'gpu10', 'gpu11', 'gpu12', 'gpu13', 'gpu14', 'gpu15')) \n",
        "    Doc:  Initialize the gpu device to use, works only if device=cpu. Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU. It can be used to run GPU-specific tests on a particular GPU.\n",
        "    Value:  \n",
        "\n",
        "force_device (<function booltype at 0x2eee578>) \n",
        "    Doc:  Raise an error if we can't use the specified device\n",
        "    Value:  False\n",
        "\n",
        "mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE')) \n",
        "    Doc:  Default compilation mode\n",
        "    Value:  Mode\n",
        "\n",
        "linker (('c|py', 'py', 'c', 'c|py_nogc', 'c&py', 'vm', 'cvm', 'vm_nogc', 'cvm_nogc')) \n",
        "    Doc:  Default linker used if the theano flags mode is Mode or ProfileMode\n",
        "    Value:  c|py\n",
        "\n",
        "optimizer (('fast_run', 'merge', 'fast_compile', 'None')) \n",
        "    Doc:  Default optimizer. If not None, will use this linker with the Mode object (not ProfileMode or DebugMode)\n",
        "    Value:  fast_run\n",
        "\n",
        "on_opt_error (('warn', 'raise')) \n",
        "    Doc:  What to do when an optimization crashes: warn and skip it, or raise the exception\n",
        "    Value:  warn\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x2ef0150>\n",
        "    Doc:  This config option was removed in 0.5: do not use it!\n",
        "    Value:  True\n",
        "\n",
        "nocleanup (<function booltype at 0x2eee1b8>) \n",
        "    Doc:  Suppress the deletion of code files that did not compile cleanly\n",
        "    Value:  False\n",
        "\n",
        "on_unused_input (('raise', 'warn', 'ignore')) \n",
        "    Doc:  What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.\n",
        "    Value:  raise\n",
        "\n",
        "tensor.cmp_sloppy (<type 'int'>) \n",
        "    Doc:  Relax tensor._allclose (0) not at all, (1) a bit, (2) more\n",
        "    Value:  0\n",
        "\n",
        "tensor.local_elemwise_fusion (<function booltype at 0x2ef2668>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "gpu.local_elemwise_fusion (<function booltype at 0x2ef27d0>) \n",
        "    Doc:  Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization\n",
        "    Value:  True\n",
        "\n",
        "lib.amdlibm (<function booltype at 0x2ef2938>) \n",
        "    Doc:  Use amd's amdlibm numerical library\n",
        "    Value:  False\n",
        "\n",
        "op.set_flops (<function booltype at 0x2ef2aa0>) \n",
        "    Doc:  currently used only in ConvOp. The profile mode will print the flops/s for the op.\n",
        "    Value:  False\n",
        "\n",
        "gpuelemwise.sync (<function booltype at 0x2ef2c08>) \n",
        "    Doc:  when true, wait that the gpu fct finished and check it error code.\n",
        "    Value:  True\n",
        "\n",
        "traceback.limit (<type 'int'>) \n",
        "    Doc:  The number of stack to trace. -1 mean all.\n",
        "    Value:  5\n",
        "\n",
        "experimental.mrg (<function booltype at 0x2ef2de8>) \n",
        "    Doc:  Another random number generator that work on the gpu\n",
        "    Value:  False\n",
        "\n",
        "numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None')) \n",
        "    Doc:  (\"Sets numpy's behaviour for floating-point errors, \", \"see numpy.seterr. 'None' means not to change numpy's default, which can be different for different numpy releases. This flag sets the default behaviour for all kinds of floating-point errors, its effect can be overriden for specific errors by the following flags: seterr_divide, seterr_over, seterr_under and seterr_invalid.\")\n",
        "    Value:  ignore\n",
        "\n",
        "numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for division by zero, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point overflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for floating-point underflow, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log')) \n",
        "    Doc:  Sets numpy's behavior for invalid floating-point operation, see numpy.seterr. 'None' means using the default, defined by numpy.seterr_all.\n",
        "    Value:  None\n",
        "\n",
        "warn.ignore_bug_before (('None', 'all', '0.3', '0.4', '0.4.1', '0.5')) \n",
        "    Doc:  If 'None', we warn about all Theano bugs found by default. If 'all', we don't warn about Theano bugs found by default. If a version, we print only the warnings relative to Theano bugs found after that version. Warning for specific bugs can be configured with specific [warn] flags.\n",
        "    Value:  None\n",
        "\n",
        "warn.argmax_pushdown_bug (<function booltype at 0x2efc320>) \n",
        "    Doc:  Warn if in past version of Theano we generated a bug with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization. Was fixed 27 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.gpusum_01_011_0111_bug (<function booltype at 0x2efc488>) \n",
        "    Doc:  Warn if we are in a case where old version of Theano had a silent bug with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096. Was fixed 31 may 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_sum_bug (<function booltype at 0x2efc5f0>) \n",
        "    Doc:  Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 august 2010 (fixed date), generated an error in that case. This happens when there are 2 consecutive sums in the graph, bad code was generated. Was fixed 2 August 2010\n",
        "    Value:  True\n",
        "\n",
        "warn.sum_div_dimshuffle_bug (<function booltype at 0x2efc758>) \n",
        "    Doc:  Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03) would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.\n",
        "    Value:  True\n",
        "\n",
        "warn.subtensor_merge_bug (<function booltype at 0x2efc8c0>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).\n",
        "    Value:  True\n",
        "\n",
        "warn.gpu_set_subtensor1 (<function booltype at 0x2efca28>) \n",
        "    Doc:  Warn if previous versions of Theano (before 0.6) could have given incorrect results when moving to the gpu set_subtensor(x[int vector], new_value)\n",
        "    Value:  True\n",
        "\n",
        "compute_test_value (('off', 'ignore', 'warn', 'raise')) \n",
        "    Doc:  If 'True', Theano will run each op at graph build time, using Constants, SharedVariables and the tag 'test_value' as inputs to the function. This helps the user track down problems in the graph before it gets optimized.\n",
        "    Value:  off\n",
        "\n",
        "exception_verbosity (('low', 'high')) \n",
        "    Doc:  If 'low', the text of exceptions will generally refer to apply nodes with short names such as Elemwise{add_no_inplace}. If 'high', some exceptions will also refer to apply nodes with long descriptions  like:\n",
        "        A. Elemwise{add_no_inplace}\n",
        "                B. log_likelihood_v_given_h\n",
        "                C. log_likelihood_h\n",
        "    Value:  low\n",
        "\n",
        "gcc.cxxflags (<type 'str'>) \n",
        "    Doc:  Extra compiler flags for gcc\n",
        "    Value:  \n",
        "\n",
        "compiledir_format (<type 'str'>) \n",
        "    Doc:  Format string for platform-dependent compiled module subdirectory\n",
        "(relative to base_compiledir). Available keys: theano_version,\n",
        "platform, numpy_version, processor, python_version. Defaults to\n",
        "'compiledir_%(platform)s-%(processor)s-%(python_version)s'.\n",
        "    Value:  compiledir_%(platform)s-%(processor)s-%(python_version)s\n",
        "\n",
        "base_compiledir (<type 'str'>) \n",
        "    Doc:  platform-independent root directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task5\n",
        "\n",
        "<theano.configparser.ConfigParam object at 0x305ecd0>\n",
        "    Doc:  platform-dependent cache directory for compiled modules\n",
        "    Value:  /home/slinderman/.theano/task5/compiledir_Linux-2.6.32-279.14.1.el6.x86_64-x86_64-with-centos-6.5-Final-x86_64-2.7.2\n",
        "\n",
        "cmodule.mac_framework_link (<function booltype at 0x33c5140>) \n",
        "    Doc:  If set to True, breaks certain MacOS installations with the infamous Bus Error\n",
        "    Value:  False\n",
        "\n",
        "cmodule.warn_no_version (<function booltype at 0x33c5398>) \n",
        "    Doc:  If True, will print a warning when compiling one or more Op with C code that can't be cached because there is no c_code_cache_version() function associated to at least one of those Ops.\n",
        "    Value:  False\n",
        "\n",
        "time_seq_optimizer (<function booltype at 0x3462488>) \n",
        "    Doc:  Should SeqOptimizer print the time taked by each of its optimizer\n",
        "    Value:  False\n",
        "\n",
        "time_eq_optimizer (<function booltype at 0x34625f0>) \n",
        "    Doc:  Should EquilibriumOptimizer print the time taken by each optimizer\n",
        "    Value:  False\n",
        "\n",
        "optdb.position_cutoff (<type 'float'>) \n",
        "    Doc:  Where to stop eariler during optimization. It represent the position of the optimizer where to stop.\n",
        "    Value:  inf\n",
        "\n",
        "optdb.max_use_ratio (<type 'float'>) \n",
        "    Doc:  A ratio that prevent infinite loop in EquilibriumOptimizer.\n",
        "    Value:  5.0\n",
        "\n",
        "profile (<function booltype at 0x3469e60>) \n",
        "    Doc:  If VM should collect profile information\n",
        "    Value:  False\n",
        "\n",
        "optimizer_excluding (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_including (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will add optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "optimizer_requiring (<type 'str'>) \n",
        "    Doc:  When using the default mode, we will require optimizer with these tags. Separate tags with ':'.\n",
        "    Value:  \n",
        "\n",
        "DebugMode.patience (<type 'int'>) \n",
        "    Doc:  Optimize graph this many times to detect inconsistency\n",
        "    Value:  10\n",
        "\n",
        "DebugMode.check_c (<function booltype at 0x32d7230>) \n",
        "    Doc:  Run C implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_py (<function booltype at 0x32d7398>) \n",
        "    Doc:  Run Python implementations where possible\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_finite (<function booltype at 0x32d7500>) \n",
        "    Doc:  True -> complain about NaN/Inf results\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_strides (<type 'int'>) \n",
        "    Doc:  Check that Python- and C-produced ndarrays have same strides.  On difference: (0) - ignore, (1) warn, or (2) raise error\n",
        "    Value:  1\n",
        "\n",
        "DebugMode.warn_input_not_reused (<function booltype at 0x32d7758>) \n",
        "    Doc:  Generate a warning when destroy_map or view_map says that an op works inplace, but the op did not reuse the input for its output.\n",
        "    Value:  True\n",
        "\n",
        "DebugMode.check_preallocated_output (<type 'str'>) \n",
        "    Doc:  Test thunks with pre-allocated memory as output storage. This is a list of strings separated by \":\". Valid values are: \"initial\" (initial storage in storage map, happens with Scan),\"previous\" (previously-returned memory), \"c_contiguous\", \"f_contiguous\", \"strided\" (positive and negative strides), \"wrong_size\" (larger and smaller dimensions), and \"ALL\" (all of the above).\n",
        "    Value:  \n",
        "\n",
        "DebugMode.check_preallocated_output_ndim (<type 'int'>) \n",
        "    Doc:  When testing with \"strided\" preallocated output memory, test all combinations of strides over that number of (inner-most) dimensions. You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.\n",
        "    Value:  4\n",
        "\n",
        "profiling.time_thunks (<function booltype at 0x32e1758>) \n",
        "    Doc:  Time individual thunks when profiling\n",
        "    Value:  True\n",
        "\n",
        "ProfileMode.n_apply_to_print (<type 'int'>) \n",
        "    Doc:  Number of apply instances to print by default\n",
        "    Value:  15\n",
        "\n",
        "ProfileMode.n_ops_to_print (<type 'int'>) \n",
        "    Doc:  Number of ops to print by default\n",
        "    Value:  20\n",
        "\n",
        "ProfileMode.min_memory_size (<type 'int'>) \n",
        "    Doc:  For the memory profile, do not print apply nodes if the size\n",
        " of their outputs (in bytes) is lower then this threshold\n",
        "    Value:  1024\n",
        "\n",
        "ProfileMode.profile_memory (<function booltype at 0x32ec230>) \n",
        "    Doc:  Enable profiling of memory used by Theano functions\n",
        "    Value:  False\n",
        "\n",
        "on_shape_error (('warn', 'raise')) \n",
        "    Doc:  warn: print a warning and use the default value. raise: raise an error\n",
        "    Value:  warn\n",
        "\n",
        "tensor.insert_inplace_optimizer_validate_nb (<type 'int'>) \n",
        "    Doc:  -1: auto, if graph have less then 500 nodes 1, else 10\n",
        "    Value:  -1\n",
        "\n",
        "experimental.local_alloc_elemwise (<function booltype at 0x3973de8>) \n",
        "    Doc:  If True enable the experimental optimization local_alloc_elemwise\n",
        "    Value:  False\n",
        "\n",
        "experimental.local_alloc_elemwise_assert (<function booltype at 0x3524488>) \n",
        "    Doc:  If False enable the experimental optimization local_alloc_elemwise but WITHOUT assert into the graph!\n",
        "    Value:  True\n",
        "\n",
        "blas.ldflags (<type 'str'>) \n",
        "    Doc:  lib[s] to include for [Fortran] level-3 blas implementation\n",
        "    Value:  -L/software/linux/x86_64/epd-7.1-2/lib -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread\n",
        "\n",
        "warn.identify_1pexp_bug (<function booltype at 0x3a0c668>) \n",
        "    Doc:  Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result due to a bug in the is_1pexp function\n",
        "    Value:  True\n",
        "\n",
        "unittests.rseed (<type 'str'>) \n",
        "    Doc:  Seed to use for randomized unit tests. Special value 'random' means using a seed of None.\n",
        "    Value:  666\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from test.parallel_coord_descent import parallel_coord_descent\n",
      "x_inf = parallel_coord_descent(client, data['N'], maxiter=1)\n",
      "\n",
      "ll_inf = popn.compute_log_p(x_inf)\n",
      "print \"LL_inf: %f\" % ll_inf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coordinate descent iteration 1.\n",
        "[stdout:0] \n",
        "Newton iter 0.\tNeuron 0. LL: 889.4\n",
        "Newton iter 1.\tNeuron 0. LL: 961.1\n",
        "Newton iter 2.\tNeuron 0. LL: 1064.6\n",
        "Newton iter 3.\tNeuron 0. LL: 1628.6\n",
        "Newton iter 4.\tNeuron 0. LL: 1716.1\n",
        "Newton iter 5.\tNeuron 0. LL: 1898.8\n",
        "Newton iter 6.\tNeuron 0. LL: 1958.2\n",
        "Newton iter 7.\tNeuron 0. LL: 2021.5\n",
        "Newton iter 8.\tNeuron 0. LL: 2076.1\n",
        "Newton iter 9.\tNeuron 0. LL: 2077.0\n",
        "Newton iter 10.\tNeuron 0. LL: 2080.4\n",
        "Newton iter 11.\tNeuron 0. LL: 2080.8\n",
        "Newton iter 12.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 13.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 14.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 15.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 16.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 17.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 18.\tNeuron 0. LL: 2080.9\n",
        "Newton iter 19.\tNeuron 0. LL: 2080.9\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -2080.927487\n",
        "         Iterations: 20\n",
        "         Function evaluations: 24\n",
        "         Gradient evaluations: 118\n",
        "         Hessian evaluations: 0\n",
        "Newton iter 0.\tNeuron 1. LL: 816.5\n",
        "Newton iter 1.\tNeuron 1. LL: 1341.2\n",
        "Newton iter 2.\tNeuron 1. LL: 2069.5\n",
        "Newton iter 3.\tNeuron 1. LL: 2145.6\n",
        "Newton iter 4.\tNeuron 1. LL: 2352.2\n",
        "Newton iter 5.\tNeuron 1. LL: 2368.0\n",
        "Newton iter 6.\tNeuron 1. LL: 2381.4\n",
        "Newton iter 7.\tNeuron 1. LL: 2384.6\n",
        "Newton iter 8.\tNeuron 1. LL: 2385.6\n",
        "Newton iter 9.\tNeuron 1. LL: 2386.1\n",
        "Newton iter 10.\tNeuron 1. LL: 2386.1\n",
        "Newton iter 11.\tNeuron 1. LL: 2386.1\n",
        "Newton iter 12.\tNeuron 1. LL: 2386.1\n",
        "Newton iter 13.\tNeuron 1. LL: 2386.1\n",
        "Newton iter 14.\tNeuron 1. LL: 2386.1\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -2386.135929\n",
        "         Iterations: 15\n",
        "         Function evaluations: 16\n",
        "         Gradient evaluations: 77\n",
        "         Hessian evaluations: 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:1] \n",
        "Newton iter 0.\tNeuron 2. LL: -75.7\n",
        "Newton iter 1.\tNeuron 2. LL: 113.3\n",
        "Newton iter 2.\tNeuron 2. LL: 309.3\n",
        "Newton iter 3.\tNeuron 2. LL: 383.4\n",
        "Newton iter 4.\tNeuron 2. LL: 385.8\n",
        "Newton iter 5.\tNeuron 2. LL: 399.6\n",
        "Newton iter 6.\tNeuron 2. LL: 399.7\n",
        "Newton iter 7.\tNeuron 2. LL: 400.1\n",
        "Newton iter 8.\tNeuron 2. LL: 400.3\n",
        "Newton iter 9.\tNeuron 2. LL: 400.3\n",
        "Newton iter 10.\tNeuron 2. LL: 400.3\n",
        "Newton iter 11.\tNeuron 2. LL: 400.3\n",
        "Newton iter 12.\tNeuron 2. LL: 400.3\n",
        "Newton iter 13.\tNeuron 2. LL: 400.3\n",
        "Newton iter 14.\tNeuron 2. LL: 400.3\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -400.260903\n",
        "         Iterations: 15\n",
        "         Function evaluations: 16\n",
        "         Gradient evaluations: 87\n",
        "         Hessian evaluations: 0\n",
        "Newton iter 0.\tNeuron 3. LL: 1071.1\n",
        "Newton iter 1.\tNeuron 3. LL: 1694.7\n",
        "Newton iter 2.\tNeuron 3. LL: 1755.7\n",
        "Newton iter 3.\tNeuron 3. LL: 1882.8\n",
        "Newton iter 4.\tNeuron 3. LL: 1888.4\n",
        "Newton iter 5.\tNeuron 3. LL: 1895.6\n",
        "Newton iter 6.\tNeuron 3. LL: 1896.9\n",
        "Newton iter 7.\tNeuron 3. LL: 1897.2\n",
        "Newton iter 8.\tNeuron 3. LL: 1897.3\n",
        "Newton iter 9.\tNeuron 3. LL: 1897.3\n",
        "Newton iter 10.\tNeuron 3. LL: 1897.3\n",
        "Newton iter 11.\tNeuron 3. LL: 1897.3\n",
        "Newton iter 12.\tNeuron 3. LL: 1897.3\n",
        "Newton iter 13.\tNeuron 3. LL: 1897.3\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -1897.338089\n",
        "         Iterations: 14\n",
        "         Function evaluations: 15\n",
        "         Gradient evaluations: 82\n",
        "         Hessian evaluations: 0\n",
        "[stdout:2] \n",
        "Newton iter 0.\tNeuron 4. LL: 1899.9\n",
        "Newton iter 1.\tNeuron 4. LL: 2364.9\n",
        "Newton iter 2.\tNeuron 4. LL: 3103.2\n",
        "Newton iter 3.\tNeuron 4. LL: 3225.0\n",
        "Newton iter 4.\tNeuron 4. LL: 3340.4\n",
        "Newton iter 5.\tNeuron 4. LL: 3377.6\n",
        "Newton iter 6.\tNeuron 4. LL: 3381.5\n",
        "Newton iter 7.\tNeuron 4. LL: 3382.7\n",
        "Newton iter 8.\tNeuron 4. LL: 3382.8\n",
        "Newton iter 9.\tNeuron 4. LL: 3382.8\n",
        "Newton iter 10.\tNeuron 4. LL: 3382.8\n",
        "Newton iter 11.\tNeuron 4. LL: 3382.8\n",
        "Newton iter 12.\tNeuron 4. LL: 3382.8\n",
        "Newton iter 13.\tNeuron 4. LL: 3382.8\n",
        "Newton iter 14.\tNeuron 4. LL: 3382.8\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -3382.790163\n",
        "         Iterations: 15\n",
        "         Function evaluations: 16\n",
        "         Gradient evaluations: 91\n",
        "         Hessian evaluations: 0\n",
        "[stdout:3] \n",
        "Newton iter 0.\tNeuron 5. LL: 1097.2\n",
        "Newton iter 1.\tNeuron 5. LL: 1281.7\n",
        "Newton iter 2.\tNeuron 5. LL: 1293.6\n",
        "Newton iter 3.\tNeuron 5. LL: 1351.2\n",
        "Newton iter 4.\tNeuron 5. LL: 1352.0\n",
        "Newton iter 5.\tNeuron 5. LL: 1356.2\n",
        "Newton iter 6.\tNeuron 5. LL: 1356.8\n",
        "Newton iter 7.\tNeuron 5. LL: 1357.1\n",
        "Newton iter 8.\tNeuron 5. LL: 1357.1\n",
        "Newton iter 9.\tNeuron 5. LL: 1357.1\n",
        "Newton iter 10.\tNeuron 5. LL: 1357.1\n",
        "Newton iter 11.\tNeuron 5. LL: 1357.1\n",
        "Newton iter 12.\tNeuron 5. LL: 1357.1\n",
        "Newton iter 13.\tNeuron 5. LL: 1357.1\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -1357.086800\n",
        "         Iterations: 14\n",
        "         Function evaluations: 15\n",
        "         Gradient evaluations: 86\n",
        "         Hessian evaluations: 0\n",
        "[stdout:4] \n",
        "Newton iter 0.\tNeuron 6. LL: -323.2\n",
        "Newton iter 1.\tNeuron 6. LL: 302.1\n",
        "Newton iter 2.\tNeuron 6. LL: 1077.3\n",
        "Newton iter 3.\tNeuron 6. LL: 1165.6\n",
        "Newton iter 4.\tNeuron 6. LL: 1467.2\n",
        "Newton iter 5.\tNeuron 6. LL: 1502.8\n",
        "Newton iter 6.\tNeuron 6. LL: 1536.8\n",
        "Newton iter 7.\tNeuron 6. LL: 1542.1\n",
        "Newton iter 8.\tNeuron 6. LL: 1543.3\n",
        "Newton iter 9.\tNeuron 6. LL: 1543.7\n",
        "Newton iter 10.\tNeuron 6. LL: 1543.9\n",
        "Newton iter 11.\tNeuron 6. LL: 1543.9\n",
        "Newton iter 12.\tNeuron 6. LL: 1543.9\n",
        "Newton iter 13.\tNeuron 6. LL: 1543.9\n",
        "Newton iter 14.\tNeuron 6. LL: 1543.9\n",
        "Newton iter 15.\tNeuron 6. LL: 1543.9\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -1543.882081\n",
        "         Iterations: 16\n",
        "         Function evaluations: 17\n",
        "         Gradient evaluations: 92\n",
        "         Hessian evaluations: 0\n",
        "[stdout:5] \n",
        "Newton iter 0.\tNeuron 7. LL: 2079.5\n",
        "Newton iter 1.\tNeuron 7. LL: 2195.8\n",
        "Newton iter 2.\tNeuron 7. LL: 2201.2\n",
        "Newton iter 3.\tNeuron 7. LL: 2215.9\n",
        "Newton iter 4.\tNeuron 7. LL: 2222.2\n",
        "Newton iter 5.\tNeuron 7. LL: 2222.4\n",
        "Newton iter 6.\tNeuron 7. LL: 2222.5\n",
        "Newton iter 7.\tNeuron 7. LL: 2222.5\n",
        "Newton iter 8.\tNeuron 7. LL: 2222.5\n",
        "Newton iter 9.\tNeuron 7. LL: 2222.5\n",
        "Newton iter 10.\tNeuron 7. LL: 2222.5\n",
        "Newton iter 11.\tNeuron 7. LL: 2222.5\n",
        "Newton iter 12.\tNeuron 7. LL: 2222.5\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -2222.505954\n",
        "         Iterations: 13\n",
        "         Function evaluations: 15\n",
        "         Gradient evaluations: 85\n",
        "         Hessian evaluations: 0\n",
        "[stdout:6] \n",
        "Newton iter 0.\tNeuron 8. LL: 1012.4\n",
        "Newton iter 1.\tNeuron 8. LL: 1226.5\n",
        "Newton iter 2.\tNeuron 8. LL: 1319.1\n",
        "Newton iter 3.\tNeuron 8. LL: 1670.8\n",
        "Newton iter 4.\tNeuron 8. LL: 1805.8\n",
        "Newton iter 5.\tNeuron 8. LL: 1855.7\n",
        "Newton iter 6.\tNeuron 8. LL: 1856.4\n",
        "Newton iter 7.\tNeuron 8. LL: 1865.1\n",
        "Newton iter 8.\tNeuron 8. LL: 1865.1\n",
        "Newton iter 9.\tNeuron 8. LL: 1865.3\n",
        "Newton iter 10.\tNeuron 8. LL: 1865.3\n",
        "Newton iter 11.\tNeuron 8. LL: 1865.4\n",
        "Newton iter 12.\tNeuron 8. LL: 1865.4\n",
        "Newton iter 13.\tNeuron 8. LL: 1865.4\n",
        "Newton iter 14.\tNeuron 8. LL: 1865.4\n",
        "Newton iter 15.\tNeuron 8. LL: 1865.4\n",
        "Newton iter 16.\tNeuron 8. LL: 1865.4\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -1865.363401\n",
        "         Iterations: 17\n",
        "         Function evaluations: 19\n",
        "         Gradient evaluations: 89\n",
        "         Hessian evaluations: 0\n",
        "[stdout:7] \n",
        "Newton iter 0.\tNeuron 9. LL: 977.9\n",
        "Newton iter 1.\tNeuron 9. LL: 1062.7\n",
        "Newton iter 2.\tNeuron 9. LL: 1502.5\n",
        "Newton iter 3.\tNeuron 9. LL: 1590.1\n",
        "Newton iter 4.\tNeuron 9. LL: 1624.3\n",
        "Newton iter 5.\tNeuron 9. LL: 1631.4\n",
        "Newton iter 6.\tNeuron 9. LL: 1633.2\n",
        "Newton iter 7.\tNeuron 9. LL: 1633.9\n",
        "Newton iter 8.\tNeuron 9. LL: 1633.9\n",
        "Newton iter 9.\tNeuron 9. LL: 1633.9\n",
        "Newton iter 10.\tNeuron 9. LL: 1633.9\n",
        "Newton iter 11.\tNeuron 9. LL: 1633.9\n",
        "Newton iter 12.\tNeuron 9. LL: 1633.9\n",
        "Newton iter 13.\tNeuron 9. LL: 1633.9\n",
        "Optimization terminated successfully.\n",
        "         Current function value: -1633.939250\n",
        "         Iterations: 14\n",
        "         Function evaluations: 15\n",
        "         Gradient evaluations: 82\n",
        "         Hessian evaluations: 0\n",
        "Iteration 1: LP=18770.23. Change in LP: 20955.05"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "LL_inf: 18770.230056"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from plotting.plot_results import plot_results\n",
      "\n",
      "x_true = None\n",
      "if 'vars' in data:\n",
      "    x_true = data['vars']\n",
      "plot_results(popn, x_inf, x_true=x_true)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Plotting connectivity matrix\n",
        "Plotting stimulus response functions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Plotting impulse response functions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Plotting firing rates"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    }
   ],
   "metadata": {}
  }
 ]
}